{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "data = data.lower()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))\n",
    "chars.append('M')  # Add the '[MASK]' token to your list of characters\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "data = torch.tensor([char_to_ix[ch] for ch in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data, context_length):\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx*self.context_length:(idx+1)*self.context_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size, d_k): # d_k dimension of embed vector\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(d_k, head_size)\n",
    "        self.keys = nn.Linear(d_k, head_size)\n",
    "        self.values = nn.Linear(d_k, head_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, d_k = x.shape  # B--> batch size, C --> con_length\n",
    "        q = self.query(x) #(B, C, H) H--> Head size\n",
    "        k = self.query(x) #(B, C, H)\n",
    "        v = self.values(x) #(B, C, H)\n",
    "\n",
    "        score = q @ k.transpose(-2, -1) * (d_k ** -0.5)  #(B, C, H)\n",
    "        prob = F.softmax(score, dim=-1)\n",
    "        out = prob @ v # (B, C, C) @ (B, C, H) = (B, C, H)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size, d_k, n_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList(SelfAttention(head_size, d_k) for _ in range(n_heads))\n",
    "        self.res_fc = nn.Linear(n_heads * head_size, d_k)\n",
    "    def forward(self, x):\n",
    "        heads = [h(x) for h in self.heads]\n",
    "        heads_concat = torch.concat(heads, dim=-1)  #(B, C, n_heads * head_size)\n",
    "        out = self.res_fc(heads_concat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "\n",
    "    def __init__(self, d_k):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc = nn.Linear(d_k, 4 * d_k) # 4 is in the original paper\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * d_k, d_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc2(self.gelu(self.fc(x)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_k, n_heads):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.encoder = MultiheadAttention(head_size=d_k // n_heads, d_k=d_k, n_heads=n_heads)\n",
    "        self.layernorm_pre_encoder = nn.LayerNorm(d_k)\n",
    "        self.fc = FC(d_k)\n",
    "        self.layernorm_pre_fc = nn.LayerNorm(d_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoder(self.layernorm_pre_encoder(x))    # Adding residual connection\n",
    "        x = x + self.fc(self.layernorm_pre_fc(x))        # Adding residual connection\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 context_length, \n",
    "                 d_k, \n",
    "                 n_heads,\n",
    "                 n_layers, \n",
    "                 device):\n",
    "\n",
    "        super(Bert, self).__init__()\n",
    "        self.device = device\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_k)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, d_k)\n",
    "        self.encoder = nn.Sequential(*[EncoderBlock(d_k, n_heads) for _ in range(n_layers)])\n",
    "        self.output = nn.Linear(d_k, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x --> (B, C)\n",
    "        B, C = x.shape\n",
    "        token_embed = self.embeddings(x) #(B, C, d_k)\n",
    "        pos_embed = self.positional_embeddings(torch.arange(C, device=self.device)) #(C, d_k)\n",
    "        x = token_embed + pos_embed # (B, C, d_k)\n",
    "        x = self.encoder(x) \n",
    "        logits = self.output(x)\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 16\n",
    "num_heads = 8\n",
    "n_layers = 3\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "context_length = 100\n",
    "mask_prob = 0.15\n",
    "device = \"mps\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert(vocab_size, context_length, embed_size, num_heads, n_layers, device)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=char_to_ix['M'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleDataset(data, context_length)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arindams_mac_m2_pro/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.7160\n",
      "Epoch [2/50], Loss: 0.5090\n",
      "Epoch [3/50], Loss: 0.4838\n",
      "Epoch [4/50], Loss: 0.4926\n",
      "Epoch [5/50], Loss: 0.4669\n",
      "Epoch [6/50], Loss: 0.5041\n",
      "Epoch [7/50], Loss: 0.4544\n",
      "Epoch [8/50], Loss: 0.4670\n",
      "Epoch [9/50], Loss: 0.4216\n",
      "Epoch [10/50], Loss: 0.4245\n",
      "Epoch [11/50], Loss: 0.4866\n",
      "Epoch [12/50], Loss: 0.4712\n",
      "Epoch [13/50], Loss: 0.4501\n",
      "Epoch [14/50], Loss: 0.4685\n",
      "Epoch [15/50], Loss: 0.4560\n",
      "Epoch [16/50], Loss: 0.4832\n",
      "Epoch [17/50], Loss: 0.4749\n",
      "Epoch [18/50], Loss: 0.4470\n",
      "Epoch [19/50], Loss: 0.4954\n",
      "Epoch [20/50], Loss: 0.4583\n",
      "Epoch [21/50], Loss: 0.4564\n",
      "Epoch [22/50], Loss: 0.4805\n",
      "Epoch [23/50], Loss: 0.4797\n",
      "Epoch [24/50], Loss: 0.4676\n",
      "Epoch [25/50], Loss: 0.4274\n",
      "Epoch [26/50], Loss: 0.4591\n",
      "Epoch [27/50], Loss: 0.4041\n",
      "Epoch [28/50], Loss: 0.4806\n",
      "Epoch [29/50], Loss: 0.4949\n",
      "Epoch [30/50], Loss: 0.4737\n",
      "Epoch [31/50], Loss: 0.4287\n",
      "Epoch [32/50], Loss: 0.5084\n",
      "Epoch [33/50], Loss: 0.4275\n",
      "Epoch [34/50], Loss: 0.4475\n",
      "Epoch [35/50], Loss: 0.4667\n",
      "Epoch [36/50], Loss: 0.4334\n",
      "Epoch [37/50], Loss: 0.4787\n",
      "Epoch [38/50], Loss: 0.4812\n",
      "Epoch [39/50], Loss: 0.4948\n",
      "Epoch [40/50], Loss: 0.4604\n",
      "Epoch [41/50], Loss: 0.4712\n",
      "Epoch [42/50], Loss: 0.4611\n",
      "Epoch [43/50], Loss: 0.5209\n",
      "Epoch [44/50], Loss: 0.4912\n",
      "Epoch [45/50], Loss: 0.4243\n",
      "Epoch [46/50], Loss: 0.5028\n",
      "Epoch [47/50], Loss: 0.4893\n",
      "Epoch [48/50], Loss: 0.4276\n",
      "Epoch [49/50], Loss: 0.4455\n",
      "Epoch [50/50], Loss: 0.4221\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        inputs = batch.clone().to(device)\n",
    "        targets = batch.clone().to(device)\n",
    "\n",
    "        # Mask some of the tokens\n",
    "        mask = torch.rand(inputs.shape) < mask_prob\n",
    "        inputs[mask] = char_to_ix['M']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the model\n",
    "torch.save(model.state_dict(), 'bert_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Bert(vocab_size, context_length, embed_size, num_heads, n_layers, device=\"cpu\")\n",
    "model.load_state_dict(torch.load(\"bert_v1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 40])\n",
      "tensor([[33,  0,  5, 28, 35]])\n",
      "first\n"
     ]
    }
   ],
   "source": [
    "model_cpu = model.to(device=\"cpu\")\n",
    "with torch.no_grad():\n",
    "    # Create input sequence with a masked token\n",
    "    input_text = \"firsM\"\n",
    "    inputs = torch.tensor([char_to_ix[ch] for ch in input_text])\n",
    "    inputs = inputs.view(1, len(inputs))\n",
    "    inputs = inputs.cpu()\n",
    "    # Get model outputs\n",
    "    outputs = model_cpu(inputs)\n",
    "    print(outputs.shape)\n",
    "    \n",
    "    # Get predicted tokens\n",
    "    _, predicted = torch.max(outputs, dim=2)\n",
    "    print(predicted)\n",
    "    \n",
    "    # Print predicted sequence\n",
    "    print(''.join([ix_to_char[ix.item()] for ix in predicted[0]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
